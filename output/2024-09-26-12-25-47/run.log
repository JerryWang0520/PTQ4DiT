09/26/2024 12:25:47 - INFO - __main__ -   Arguments: Namespace(model='DiT-XL/2', vae='mse', image_size=256, num_classes=1000, cfg_scale=1.5, num_sampling_steps=50, seed=1, ckpt='/data/wujunyi/code/quantization/Gen/DiT_BRECQ_CRCV_smooth_FinalLayer/pretrained_models/DiT-XL-2-256x256.pt', outdir='output/', ptq=True, weight_bit=8, act_bit=8, cali_ckpt=None, cali_data_path='/data/wujunyi/code/quantization/Gen/imagenet_DiT-256_sample4000_50steps_allst.pt', resume=False, cali_st=25, cali_n=64, cali_iters=20000, cali_batch_size=32, cali_lr=0.0004, cali_p=2.4, sm_abit=8, recon=False, opt_mode='mse', sample=True, inference=False, n_c=10, c_begin=0, c_end=999)
09/26/2024 12:25:47 - INFO - __main__ -   Saving to output/2024-09-26-12-25-47
09/26/2024 12:25:57 - INFO - __main__ -   Loaded DiT-XL/2 with 675,129,632 parameters.
09/26/2024 12:26:00 - INFO - __main__ -   QuantModel(
  (model): DiT(
    (x_embedder): PatchEmbed(
      (proj): QuantModule(
        4, 1152, kernel_size=(2, 2), stride=(2, 2)
        (weight_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=True)
        (act_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=False)
        (activation_function): StraightThrough()
      )
      (norm): Identity()
    )
    (t_embedder): TimestepEmbedder(
      (mlp): Sequential(
        (0): QuantModule(
          in_features=256, out_features=1152, bias=True
          (weight_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=True)
          (act_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=False)
          (activation_function): StraightThrough()
        )
        (1): SiLU()
        (2): QuantModule(
          in_features=1152, out_features=1152, bias=True
          (weight_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=True)
          (act_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=False)
          (activation_function): StraightThrough()
        )
      )
    )
    (y_embedder): LabelEmbedder(
      (embedding_table): Embedding(1001, 1152)
    )
    (blocks): ModuleList(
      (0-27): 28 x QuantDiTBlock(
        (act_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=False)
        (activation_function): StraightThrough()
        (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)
        (attn): Attention(
          (qkv): QuantModule(
            in_features=1152, out_features=3456, bias=True
            (weight_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=True)
            (act_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=False)
            (activation_function): StraightThrough()
          )
          (q_norm): Identity()
          (k_norm): Identity()
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): QuantModule(
            in_features=1152, out_features=1152, bias=True
            (weight_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=True)
            (act_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=False)
            (activation_function): StraightThrough()
          )
          (proj_drop): Dropout(p=0.0, inplace=False)
          (act_quantizer_q): (UniformAffineQuantizer() inited=False, channel_wise=False)
          (act_quantizer_k): (UniformAffineQuantizer() inited=False, channel_wise=False)
          (act_quantizer_v): (UniformAffineQuantizer() inited=False, channel_wise=False)
          (act_quantizer_w): (UniformAffineQuantizer() inited=False, channel_wise=False)
        )
        (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)
        (mlp): Mlp(
          (fc1): QuantModule(
            in_features=1152, out_features=4608, bias=True
            (weight_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=True)
            (act_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=False)
            (activation_function): StraightThrough()
          )
          (act): GELU(approximate='tanh')
          (drop1): Dropout(p=0, inplace=False)
          (norm): Identity()
          (fc2): QuantModule(
            in_features=4608, out_features=1152, bias=True
            (weight_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=True)
            (act_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=False)
            (activation_function): StraightThrough()
          )
          (drop2): Dropout(p=0, inplace=False)
        )
        (adaLN_modulation): Sequential(
          (0): SiLU()
          (1): QuantModule(
            in_features=1152, out_features=6912, bias=True
            (weight_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=True)
            (act_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=False)
            (activation_function): StraightThrough()
          )
        )
      )
    )
    (final_layer): QuantFinalLayer(
      (act_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=False)
      (activation_function): StraightThrough()
      (norm_final): LayerNorm((1152,), eps=1e-06, elementwise_affine=False)
      (linear): QuantModule(
        in_features=1152, out_features=32, bias=True
        (weight_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=True)
        (act_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=False)
        (activation_function): StraightThrough()
      )
      (adaLN_modulation): Sequential(
        (0): SiLU()
        (1): QuantModule(
          in_features=1152, out_features=2304, bias=True
          (weight_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=True)
          (act_quantizer): (UniformAffineQuantizer() inited=False, channel_wise=False)
          (activation_function): StraightThrough()
        )
      )
    )
  )
)
09/26/2024 12:26:00 - INFO - __main__ -   Sampling data from 25 timesteps for calibration
09/26/2024 12:26:01 - INFO - quant.utils -   Selected 25 steps from 50 sampling steps
09/26/2024 12:26:02 - INFO - __main__ -   Calibration data shape: torch.Size([1600, 4, 32, 32]) torch.Size([1600]) torch.Size([1600])
09/26/2024 12:26:02 - INFO - __main__ -   cali_init shape: torch.Size([100, 4, 32, 32]), torch.Size([100]), torch.Size([100])
09/26/2024 12:26:02 - INFO - __main__ -   cali_init_ts: tensor([999, 999, 958, 958, 917, 917, 877, 877, 836, 836, 795, 795, 754, 754,
        714, 714, 673, 673, 632, 632, 591, 591, 550, 550, 510, 510, 469, 469,
        428, 428, 387, 387, 347, 347, 306, 306, 265, 265, 224, 224, 183, 183,
        143, 143, 102, 102,  61,  61,  20,  20, 999, 999, 958, 958, 917, 917,
        877, 877, 836, 836, 795, 795, 754, 754, 714, 714, 673, 673, 632, 632,
        591, 591, 550, 550, 510, 510, 469, 469, 428, 428, 387, 387, 347, 347,
        306, 306, 265, 265, 224, 224, 183, 183, 143, 143, 102, 102,  61,  61,
         20,  20])
09/26/2024 12:26:02 - INFO - __main__ -   cali_init_ys: tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,
           0,    0, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,
        1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,
        1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,
        1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000,
        1000, 1000, 1000, 1000])
09/26/2024 12:26:02 - INFO - __main__ -   Initializing scaling factors
